{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Compare the list of relevant words obtained for each gold standard summary to the list of most important words used for our tf-idf based summarisations.\n",
    "    \n",
    "The notebook is structured as follows:\n",
    "<ol>\n",
    "  <li>get the words that we based the english bible summaries on</li>\n",
    "  <li>definition of similarity measure function</li>\n",
    "  <li>execution of similarity measure functions on our topten words in comparisson to different goldstandard summaries</li>\n",
    "    <li>Examine german summaries</li>\n",
    "    <li>First steps of attempt for using wordNets</li>\n",
    "</ol>\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moses', 'lord', 'aaron', 'pharaoh', 'sockets', 'israel', 'people', 'egypt', 'tabernacle', 'land']\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#read the topten words used for the summaries generated by our function \n",
    "\n",
    "#with open('topten_words.txt', 'r') as f:\n",
    "#    words=f.readlines()\n",
    "with open('topten_words_summaries.txt', 'r') as f:\n",
    "    words=f.readlines()    \n",
    "#clean up the format \n",
    "#--> combine all the single chars one long string \n",
    "#--> split that again by ',' to obtain a list:\n",
    "topten_words = []\n",
    "for i in range(len(words)):\n",
    "    concat_string = ''.join(words[i]) \n",
    "    cleaned_string = re.sub('\\]','',re.sub('\\[','',concat_string))\n",
    "    topten_words.append(cleaned_string.split(','))\n",
    "\n",
    "\n",
    "# remove punctuation and \\n\n",
    "for i in range(len(topten_words)):\n",
    "    for j in range(len(topten_words[i])):\n",
    "        topten_words[i][j] = re.sub(' ', '', re.sub('\\'', '', topten_words[i][j].rstrip())).lower()\n",
    "        \n",
    "print(topten_words[1])\n",
    "print(len(topten_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joseph', 'luz', 'and', 'egyptso', 'came', 'canaan', 'land', 'went', 'bethel', 'jacob', 'people', 'name', 'what']\n"
     ]
    }
   ],
   "source": [
    "with open('words_top_tfidf_score_summaries.txt', 'r') as f:\n",
    "    words=f.readlines()    \n",
    "#clean up the format \n",
    "#--> combine all the single chars one long string \n",
    "#--> split that again by ',' to obtain a list:\n",
    "\n",
    "words_top_tfidf_scores = []\n",
    "for i in range(len(words)):\n",
    "    concat_string = ''.join(words[i]) \n",
    "    cleaned_string = re.sub('\\]','',re.sub('\\[','',concat_string))\n",
    "    words_top_tfidf_scores.append(cleaned_string.split(','))\n",
    "\n",
    "# remove punctuation and \\n and make sure we have only lower case words\n",
    "for i in range(len(words_top_tfidf_scores)):\n",
    "    for j in range(len(words_top_tfidf_scores[i])):\n",
    "        words_top_tfidf_scores[i][j] = re.sub(' ', '', re.sub('\\'', '', words_top_tfidf_scores[i][j].rstrip())).lower()\n",
    "        \n",
    "print(words_top_tfidf_scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "2. definition of similarity measure function\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareLists(gold_summary_words, topten_words):\n",
    "    #use a score calculation to get a educated comparisson measurement\n",
    "    counter_common = 0\n",
    "    diff_gold_summary_words = list(set(gold_summary_words) - set(topten_words))\n",
    "    diff_topten_words = list(set(topten_words) - set(gold_summary_words))\n",
    "    \n",
    "    same_elem = []\n",
    "    for elem in gold_summary_words:\n",
    "        if elem in topten_words:\n",
    "            same_elem.append(elem)\n",
    "\n",
    "    nr_words_gold_sum =len(gold_summary_words) \n",
    "    nr_common_words = len(same_elem)\n",
    "   # return same_elem, nr_common_words,nr_words_gold_sum, diff_gold_summary_words, diff_topten_words\n",
    "    return same_elem, nr_common_words,nr_words_gold_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_bleu(out,ref):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        out is the machine-outputted summary as list of words\n",
    "        ref is the gold_standard as a list of words\n",
    "    Output:\n",
    "        rouge = number_of_overlappting_words / total_words_in_reference_summary\n",
    "        bleu = number_of_overlappting_words / total_words_in_system_summary\n",
    "    \"\"\"\n",
    "\n",
    "    out_i = [0] * len(out)\n",
    "    ref_i = [0] * len(ref)\n",
    "    for o, olem in enumerate(out):\n",
    "        for r, rlem in enumerate(ref):\n",
    "            if olem == rlem:\n",
    "                if out_i[o] == 0:\n",
    "                    if ref_i[r] == 0:\n",
    "                        out_i[o] = 1\n",
    "                        ref_i[r] = 1\n",
    "    rouge = sum(out_i) / len(ref)\n",
    "    bleu = sum(out_i) / len(out)\n",
    "\n",
    "    return rouge, bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "3. execution of similarity measure functions on our topten words in comparisson to different goldstandard summaries\n",
    "<ul>\n",
    "  A) goldstandard summaries = first senteces of each book <br>\n",
    "  B) goldstandard summaries = summaries for each book found online at https://www.printfriendly.com/p/g/tp79hs <br>\n",
    "  C) goldstandard summaries = summaries for each chapter combined per book, found online at \n",
    "</ul>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">A) goldstandard summaries = first senteces of each book<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "with open('gold_summaries_words.txt', 'r') as f:\n",
    "    books=f.readlines()\n",
    "    \n",
    "gold_summaries_words = []\n",
    "for i in range(len(books)):\n",
    "    concat_string = ''.join(books[i]) \n",
    "    cleaned_string = re.sub('\\n','',re.sub('\\'','',re.sub(' ','',re.sub('\\]','',re.sub('\\[','',concat_string))))).lower()\n",
    "    gold_summaries_words.append(cleaned_string.split(','))\n",
    "\n",
    "\n",
    "#allways check the length\n",
    "print(len(gold_summaries_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can execute the rouge_bleu function for our topten_words and the gold standard A)\n",
    "scores_topten = []\n",
    "for n in range(len(gold_summaries_words)):\n",
    "    #gold_summaries_words[n].pop(0)\n",
    "    rouge,bleu = rouge_bleu(topten_words[n], gold_summaries_words[n])\n",
    "    scores_topten.append([n,rouge,bleu ])\n",
    "#print(scores_topten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE : 0.09375\n",
      "BLEU  : 16.600000000000005\n"
     ]
    }
   ],
   "source": [
    "len(scores_topten)\n",
    "rouge_scores_top_ten_one_sentence = 0\n",
    "bleu_scores_topten_one_sentence = 0\n",
    "\n",
    "for elem in scores_topten:\n",
    "    rouge_scores_topten_one_sentence = rouge_scores_top_ten_one_sentence+elem[1]\n",
    "    bleu_scores_topten_one_sentence = bleu_scores_topten_one_sentence+elem[2]\n",
    "print('ROUGE : '+str(rouge_scores_topten_one_sentence))\n",
    "print('BLEU  : '+str(bleu_scores_topten_one_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.0, 0.0], [1, 0.08333333333333333, 0.07692307692307693], [2, 0.13333333333333333, 0.18181818181818182], [3, 0.09615384615384616, 0.4166666666666667], [4, 0.0, 0.0], [5, 0.07692307692307693, 0.3333333333333333], [6, 0.03125, 0.1], [7, 0.1111111111111111, 0.03333333333333333], [8, 0.02702702702702703, 0.09090909090909091], [9, 0.045454545454545456, 0.2], [10, 0.09090909090909091, 0.14285714285714285], [11, 0.16666666666666666, 0.058823529411764705], [12, 0.045454545454545456, 0.02702702702702703], [13, 0.23076923076923078, 0.23076923076923078], [14, 0.020833333333333332, 0.01639344262295082], [15, 0.4, 0.03125], [16, 0.05, 0.23076923076923078], [17, 0.1111111111111111, 0.13333333333333333], [18, 0.07692307692307693, 0.25], [19, 0.06451612903225806, 0.1], [20, 0.1111111111111111, 0.16666666666666666], [21, 0.0, 0.0], [22, 0.0, 0.0], [23, 0.13043478260869565, 0.21428571428571427], [24, 0.0, 0.0], [25, 0.07692307692307693, 0.1], [26, 0.14285714285714285, 0.2], [27, 0.07142857142857142, 0.1111111111111111], [28, 0.14285714285714285, 0.037037037037037035], [29, 0.07692307692307693, 0.14285714285714285], [30, 0.0, 0.0], [31, 0.19047619047619047, 0.26666666666666666], [32, 0.05, 0.04], [33, 0.3333333333333333, 0.03125], [34, 0.0, 0.0], [35, 0.041666666666666664, 0.030303030303030304], [36, 0.09615384615384616, 0.3125], [37, 0.07692307692307693, 0.09523809523809523], [38, 0.16666666666666666, 0.08333333333333333], [39, 0.09090909090909091, 0.1], [40, 0.1, 0.125], [41, 0.0, 0.0], [42, 0.1111111111111111, 0.08333333333333333], [43, 0.03508771929824561, 0.16666666666666666], [44, 0.04411764705882353, 0.3333333333333333], [45, 0.125, 0.18518518518518517], [46, 0.13043478260869565, 0.21428571428571427], [47, 0.06521739130434782, 0.1875], [48, 0.2608695652173913, 0.35294117647058826], [49, 0.125, 0.2727272727272727], [50, 0.04, 0.06666666666666667], [51, 0.20833333333333334, 0.3125], [52, 0.22727272727272727, 0.19230769230769232], [53, 0.10344827586206896, 0.13636363636363635], [54, 0.13793103448275862, 0.2857142857142857], [55, 0.05172413793103448, 0.10714285714285714], [56, 0.15625, 0.1724137931034483], [57, 0.019230769230769232, 0.058823529411764705], [58, 0.21428571428571427, 0.14285714285714285], [59, 0.10526315789473684, 0.4], [60, 0.1568627450980392, 0.3333333333333333], [61, 0.0625, 0.23076923076923078], [62, 0.047619047619047616, 0.08333333333333333], [63, 0.25, 0.06896551724137931], [64, 0.08, 0.041666666666666664], [65, 0.1875, 0.20689655172413793]]\n"
     ]
    }
   ],
   "source": [
    "#and we can execute rouge_bleu for our top_tfidf_score words and the gold standard A)\n",
    "\n",
    "scores_top_score = []\n",
    "for n in range(len(gold_summaries_words)):\n",
    "    #gold_summaries_words[n].pop(0)\n",
    "    rouge,bleu = rouge_bleu(words_top_tfidf_scores[n], gold_summaries_words[n])\n",
    "    scores_top_score.append([n,rouge,bleu ])\n",
    "print(scores_top_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE : 6.705492392982123\n",
      "BLEU  : 9.346182335910829\n"
     ]
    }
   ],
   "source": [
    "bleu_top_scores_one_sentence = 0\n",
    "rouge_top_scores_one_sentence = 0\n",
    "\n",
    "for elem in scores_top_score:\n",
    "    rouge_top_scores_one_sentence = rouge_top_scores_one_sentence+elem[1]\n",
    "    bleu_top_scores_one_sentence = bleu_top_scores_one_sentence+elem[2]\n",
    "print('ROUGE : '+str(rouge_top_scores_one_sentence))\n",
    "print('BLEU  : '+str(bleu_top_scores_one_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "  B) goldstandard summaries = summaries for each book found online at https://www.printfriendly.com/p/g/tp79hs <br>\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Old Testament\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a gold standard from online summaries\n",
    "#the online summaries where pasted into the file\n",
    "with open('book_summaries.txt', 'r') as f:\n",
    "    words=f.readlines()\n",
    "\n",
    "filtered_list = list(filter(lambda a: a != 'New Testament\\n',filter(lambda a: a != 'Advertisements\\n',filter(lambda a: a != '\\n', words))))\n",
    "filtered_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list = []\n",
    "book_counter = 1\n",
    "\n",
    "for i in range(len(filtered_list)):\n",
    "    if i % 2 == 0 :\n",
    "        if i!=0:\n",
    "            book_nr = book_counter-1\n",
    "            concat_list.append(['BOOK'+str(book_nr),filtered_list[i]])\n",
    "        book_counter+=1\n",
    "#concat_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopwords = list(stopwords.words('english'))\n",
    "newstopwords = ['shalt', 'saith', 'thy','thee', 'thou', 'shall', 'thine', 'ye', 'you', 'whereof', 'didst', 'thereof', 'shouldest', 'unto', 'said', 'upon', 'also', 'abound', 'book64', 'book57', 'us', 'ai', 'book63', 'let', 'hereby', 'hath', 'saying', 'book65', 'doeth', 'say']\n",
    "stopwords.extend(newstopwords)\n",
    "\n",
    "diff_gold_summaries = []\n",
    "for summary in concat_list:\n",
    "    clean_summary_per_book = []\n",
    "    single_words  = summary[1].split(' ')\n",
    "    for word in single_words: \n",
    "        if word not in stopwords:\n",
    "            clean_summary_per_book.append(word)\n",
    "    clean_summary_per_book = list(dict.fromkeys(clean_summary_per_book))\n",
    "    diff_gold_summaries.append([summary[0],clean_summary_per_book])\n",
    "\n",
    "# diff_gold_summaries[0]\n",
    "\n",
    "online_gold_standard = [[word.replace('“', '').replace('\\n','').replace('?','').replace('”','').replace('’s','').lower() for word in elem[1]] for elem in diff_gold_summaries]\n",
    "online_gold_standard[1]\n",
    "len(online_gold_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can execute the rouge_bleu function for our topten_words and the gold standard B)\n",
    "scores_online_gold_standard = []\n",
    "for n in range(len(online_gold_standard)):\n",
    "    rouge,bleu = rouge_bleu(topten_words[n], online_gold_standard[n])\n",
    "    scores_online_gold_standard.append([n,rouge,bleu ])\n",
    "#scores_online_gold_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE : 0.0\n",
      "BLEU  : 5.1999999999999975\n"
     ]
    }
   ],
   "source": [
    "#scores_online_gold_standard\n",
    "rouge_scores_top_ten_online_s = 0\n",
    "bleu_scores_topten_online_s = 0\n",
    "\n",
    "for elem in scores_online_gold_standard:\n",
    "    rouge_scores_topten_online_s = rouge_scores_top_ten_online_s+elem[1]\n",
    "    bleu_scores_topten_online_s = bleu_scores_topten_online_s+elem[2]\n",
    "print('ROUGE : '+str(rouge_scores_topten_online_s))\n",
    "print('BLEU  : '+str(bleu_scores_topten_online_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and we can execute rouge_bleu for our top_tfidf_score words and the gold standard B)\n",
    "\n",
    "scores_top_score_online = []\n",
    "for n in range(len(gold_summaries_words)):\n",
    "    #gold_summaries_words[n].pop(0)\n",
    "    rouge,bleu = rouge_bleu(words_top_tfidf_scores[n], gold_summaries_words[n])\n",
    "    scores_top_score_online.append([n,rouge,bleu ])\n",
    "#print(scores_top_score_online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE : 6.705492392982123\n",
      "BLEU  : 9.346182335910829\n"
     ]
    }
   ],
   "source": [
    "#scores_online_gold_standard\n",
    "rouge_top_scores_online_s = 0\n",
    "bleu_top_scores_online_s = 0\n",
    "\n",
    "for elem in scores_top_score_online:\n",
    "    rouge_top_scores_online_s = rouge_top_scores_online_s+elem[1]\n",
    "    bleu_top_scores_online_s = bleu_top_scores_online_s+elem[2]\n",
    "print('ROUGE : '+str(rouge_top_scores_online_s))\n",
    "print('BLEU  : '+str(bleu_top_scores_online_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of compareLists() cofirms that the set of topten words of our summaries have no overlappings with the relevant words from the online summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "  \n",
    "  C) goldstandard summaries = summaries for each chapter combined per book, found online at<br>\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bible_chapter_summaries_66.txt', 'r') as f:\n",
    "    summaries_66=f.readlines()\n",
    "#summaries_66[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'revelation'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "len(summaries_66)\n",
    "type(summaries_66[0])\n",
    "cleaned_summaries_66 = []\n",
    "index_counter = 0\n",
    "for summary in summaries_66:\n",
    "    new_list = summary.split(' ')\n",
    "    cleaned_book_summary_words = []\n",
    "    #remove brackets and unwanted chars from the first and the last element of the list:\n",
    "    new_first_elem = re.sub('\\'','',re.sub('\\[','',summary[0]))\n",
    "    new_last_elem = re.sub('\\\\n','',re.sub('\\'\\]','',re.sub('\\.','',new_list[-1:][0] )))\n",
    "    \n",
    "    new_list.pop(0)\n",
    "    new_list.insert(0,new_first_elem)\n",
    "    new_list.pop()\n",
    "    new_list.append(new_last_elem)\n",
    "    \n",
    "    for elem in new_list: \n",
    "        if elem not in stopwords:\n",
    "            cleaned_book_summary_words.append(elem)\n",
    "    remove_duplicates = list(set(cleaned_book_summary_words))\n",
    "    remove_punctuation = []\n",
    "    #remove punctuation marks\n",
    "    for elem in remove_duplicates:\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        remove_punctuation.append(elem.translate(translator))\n",
    "            \n",
    "    #cleaned_summaries_66.append(cleaned_book_summary_words)\n",
    "    cleaned_summaries_66.append(remove_punctuation)\n",
    "#    if index_counter ==0:\n",
    "#        print(type(cleaned_book_summary_words))\n",
    "#        print(cleaned_book_summary_words[1])\n",
    "#        print(type(cleaned_summaries_66[0]))\n",
    "#        print(cleaned_summaries_66[0][1])\n",
    "#    index_counter += 1\n",
    "new_list[1]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE : 1.6444787963130405\n",
      "BLEU  : 19.300000000000004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Now we can execute the rouge_bleu function for our topten_words and the gold standard C)\n",
    "scores_topten_words_chapter = []\n",
    "for i in range(len(cleaned_summaries_66)):\n",
    "    rouge,bleu = rouge_bleu(topten_words[i],cleaned_summaries_66[i])\n",
    "    scores_topten_words_chapter.append([i, rouge,bleu])\n",
    "\n",
    "#scores_topten_words_chapter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_topten_allchapters_summary = 0\n",
    "bleu_topten_allchapters_summary = 0\n",
    "\n",
    "for elem in scores_topten_words_chapter:\n",
    "    rouge_topten_allchapters_summary = rouge_topten_allchapters_summary+elem[1]\n",
    "    bleu_topten_allchapters_summary = bleu_topten_allchapters_summary+elem[2]\n",
    "print('ROUGE : '+str(rouge_topten_allchapters_summary))\n",
    "print('BLEU  : '+str(bleu_topten_allchapters_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and we can execute rouge_bleu for our top_tfidf_score words and the gold standard C)\n",
    "similarity_scores_summary_from_chapter = []\n",
    "scores_top_score_chapter = []\n",
    "for i in range(len(cleaned_summaries_66)):\n",
    "    rouge, bleu= rouge_bleu(words_top_tfidf_scores[i],cleaned_summaries_66[i])\n",
    "    scores_top_score_chapter.append([i, rouge, bleu])\n",
    "#print(scores_top_score_chapter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE : 1.8044210051515361\n",
      "BLEU  : 12.162355845115012\n"
     ]
    }
   ],
   "source": [
    "rouge_top_scores_allchapters_summary = 0\n",
    "bleu_top_scores_allchapters_summary = 0\n",
    "\n",
    "for elem in scores_top_score_chapter:\n",
    "    rouge_top_scores_allchapters_summary = rouge_top_scores_allchapters_summary+elem[1]\n",
    "    bleu_top_scores_allchapters_summary = bleu_top_scores_allchapters_summary+elem[2]\n",
    "print('ROUGE : '+str(rouge_top_scores_allchapters_summary))\n",
    "print('BLEU  : '+str(bleu_top_scores_allchapters_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">4. Examine german summaries:\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the german summaries we ran out of time and could therefore not use the second approach of creating summaries based on the overall tf-idf score. Therefore, in the following cells we only compare our summaries created with the first approach to the german gold standard summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "<class 'list'>\n",
      "['schoepfung', 'vorfahren', 'einheit', 'geworden', 'dar', 'leben', 'ab', 'foerdert', 'geschichten', 'ist', 'erzaehlungen', 'erzaehlen', 'mosebuches', 'geschichte', 'welt', 'anfang', 'berichten', 'enden', 'menschheit', 'menschenpaar', 'gut', 'gott', 'einsetzt', 'abrahams', 'wollen', 'orientiert', 'einst', 'hindert', 'war', 'fuehrt', 'darin', 'heute', 'vielmehr', 'erste', 'erleben', 'turmbau', 'kapitel', 'erkennt', 'mythen', 'gen', 'warum', 'ersten', 'unserer', 'damit', 'urgeschichte', 'genannten', 'boese', 'stellen', 'dadurch', '111', 'heben', 'gesamten', 'folgenden', 'aehnlich', 'babel']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use the words for the german summaries\n",
    "\n",
    "#open the txt which contains the words from the goldstandard summaries in german:\n",
    "with open('words_german_goldstandard_summaries.txt', 'r', encoding = 'utf-8') as f:\n",
    "    words_goldstandard_german=f.readlines()    \n",
    "\n",
    "#and check the format\n",
    "print(len(words_goldstandard_german))\n",
    "print(type(words_goldstandard_german))\n",
    "print(words_goldstandard_german[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "<class 'list'>\n",
      "['sprach', 'jakob', 'abram', 'joseph', 'laban', 'abraham', 'pharao', 'herr', 'isaak']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#open the txt which contains the words from the summaries in german generated with topten tf idf:\n",
    "with open('words_topten_german_summaries.txt', 'r', encoding = 'utf-8') as f:\n",
    "    words_topten_german=f.readlines()    \n",
    "\n",
    "\n",
    "print(len(words_topten_german))\n",
    "print(type(words_topten_german))\n",
    "print(words_topten_german[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now we can execute the rouge_bleu function for our topten_words in german and and the gold standard respectively\n",
    "scores_topten_words_german = []\n",
    "for i in range(len(words_goldstandard_german)):\n",
    "    rouge,bleu = rouge_bleu(words_topten_german[i],words_goldstandard_german[i])\n",
    "    scores_topten_words_german.append([i, rouge,bleu])\n",
    "#scores_topten_words_german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE : 6.95324916914873\n",
      "BLEU  : 65.8121662307199\n"
     ]
    }
   ],
   "source": [
    "rouge_topten_one_sentence_german = 0\n",
    "bleu_topten_one_sentence_german = 0\n",
    "\n",
    "for elem in scores_topten_words_german:\n",
    "    rouge_topten_one_sentence_german = rouge_topten_one_sentence_german+elem[1]\n",
    "    bleu_topten_one_sentence_german = bleu_topten_one_sentence_german+elem[2]\n",
    "print('ROUGE : '+str(rouge_topten_one_sentence_german))\n",
    "print('BLEU  : '+str(bleu_topten_one_sentence_german))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">5. First steps of attempt for using wordNets\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we started to examine the word relatedness of our english text where we have very low ROUGE/BLEU scores. The idea is to find that even though the scores are low, there might be some common hypernyms of the words from our goldstandards and the words from summaries generated by us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#first, import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('make.v.03'),\n",
       " Synset('create.v.02'),\n",
       " Synset('create.v.03'),\n",
       " Synset('create.v.04'),\n",
       " Synset('create.v.05'),\n",
       " Synset('produce.v.02')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recap the structure of synsets by checking the synsets of \"created\"\n",
    "#created is one of the words which was not found to be common in the summaries generated by us and the gold standard\n",
    "wn.synsets('created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('create.v.02')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('create.v.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT ANSWER\n",
    "def have_hypernym(w1,w2):\n",
    "    i,j = 0,0\n",
    "    w1_all_synsets = wn.synsets(w1)\n",
    "    w2_all_synsets = wn.synsets(w2)\n",
    "    \n",
    "    word_w1,word_w2 = [],[]\n",
    "    \n",
    "    while i < len(w1_all_synsets):\n",
    "            word_w1 = w1_all_synsets[i].hypernyms()\n",
    "            for e in word_w1:\n",
    "                while j < len(w2_all_synsets):\n",
    "                    word_w2 = w2_all_synsets[j].hypernyms()\n",
    "                    for f in word_w2:\n",
    "                        if f == e:\n",
    "                            print(str(f))\n",
    "                            return True\n",
    "                    j+=1\n",
    "            i+=1\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('edible_fruit.n.01')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the function\n",
    "have_hypernym('apple','pear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the \"have_hypernym\"-funtion takes strings as input. Therefore it should work on the wordlists that we have above. But it seems unnecessary to compare words that occur in both lists that are compared. Therefore, the idea is to create two lists of words that are not common for every comparisson process ( i. e. calling the \"rouge_bleu\"-function) which can be obtained using the \"compareLists\"-function. One list would hold all words from our summaries which are not in the gold standard list and the other vice versa. We could then compare each word from both list with another and if we find a hypernym e.g. on the first level (=both words have the same direct hypernym), the word gets scored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
